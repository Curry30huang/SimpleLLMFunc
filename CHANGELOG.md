# Change log for SimpleLLMFunc

## 0.5.0 (Unreleased) - Event Stream & Type System Refactoring

### ğŸ‰ é‡å¤§åŠŸèƒ½

1. **Event Stream ç³»ç»Ÿ**: å…¨æ–°çš„å¯è§‚æµ‹æ€§ç³»ç»Ÿï¼Œæ”¯æŒå®æ—¶è§‚å¯Ÿ ReAct å¾ªç¯çš„æ‰§è¡Œè¿‡ç¨‹
   - æ–°å¢ `enable_event` å‚æ•°ï¼ˆé»˜è®¤ `False`ï¼Œä¿è¯å‘åå…¼å®¹ï¼‰
   - æ”¯æŒ 13 ç§äº‹ä»¶ç±»å‹ï¼šReAct å¼€å§‹/ç»“æŸã€LLM è°ƒç”¨ã€å·¥å…·è°ƒç”¨ã€è¿­ä»£ç­‰
   - Tagged Union è®¾è®¡ï¼Œç±»å‹å®‰å…¨ä¸”çµæ´»
   - æä¾›è¿‡æ»¤å™¨å‡½æ•°ï¼š`responses_only()`, `events_only()`, `filter_events()`
   - æä¾›è£…é¥°å™¨ï¼š`with_event_observer()` ç”¨äºäº‹ä»¶è§‚æµ‹

2. **ç±»å‹ç³»ç»Ÿé‡æ„**: ç»Ÿä¸€ç±»å‹å®šä¹‰ï¼Œæ¶ˆé™¤é‡å¤ï¼Œæå‡ç±»å‹å®‰å…¨
   - æ–°å¢ `type/tool_call.py`: å·¥å…·è°ƒç”¨ç›¸å…³ç±»å‹
   - æ–°å¢ `type/llm.py`: LLM å“åº”ç›¸å…³ç±»å‹
   - æ–°å¢ `type/hooks.py`: Hook ç³»ç»Ÿç›¸å…³ç±»å‹
   - å¤ç”¨ OpenAI SDK ç±»å‹ï¼Œå‡å°‘è‡ªå®šä¹‰ç±»å‹
   - ç»Ÿä¸€å¯¼å‡ºæ‰€æœ‰ç±»å‹åˆ° `type/__init__.py`

### âœ¨ æ–°ç‰¹æ€§

1. **äº‹ä»¶ç±»å‹ç³»ç»Ÿ**:
   - `ReactStartEvent`: ReAct å¾ªç¯å¼€å§‹
   - `LLMCallStartEvent` / `LLMCallEndEvent`: LLM è°ƒç”¨äº‹ä»¶
   - `LLMChunkArriveEvent`: æµå¼ chunk åˆ°è¾¾ï¼ˆä»…æµå¼æ¨¡å¼ï¼‰
   - `ToolCallsBatchStartEvent` / `ToolCallsBatchEndEvent`: å·¥å…·è°ƒç”¨æ‰¹æ¬¡äº‹ä»¶
   - `ToolCallStartEvent` / `ToolCallEndEvent` / `ToolCallErrorEvent`: å•ä¸ªå·¥å…·è°ƒç”¨äº‹ä»¶
   - `ReactIterationStartEvent` / `ReactIterationEndEvent`: è¿­ä»£äº‹ä»¶
   - `ReactEndEvent`: ReAct å¾ªç¯ç»“æŸ

2. **Event Stream API**:
   ```python
   @llm_chat(llm_interface=llm, enable_event=True)
   async def my_chat(message: str):
       pass
   
   # å¤„ç†äº‹ä»¶å’Œå“åº”
   async for output in my_chat("Hello"):
       if output.type == "response":
           print(output.response)
       elif output.type == "event":
           print(output.event.event_type)
   ```

3. **è¾…åŠ©å·¥å…·å‡½æ•°**:
   - `responses_only()`: åªè·å–å“åº”ï¼ˆå‘åå…¼å®¹ï¼‰
   - `events_only()`: åªè·å–äº‹ä»¶
   - `filter_events()`: è¿‡æ»¤ç‰¹å®šäº‹ä»¶ç±»å‹
   - `with_event_observer()`: æ·»åŠ äº‹ä»¶è§‚æµ‹å™¨è£…é¥°å™¨

### ğŸ”§ æ”¹è¿›

1. **ç±»å‹ç³»ç»Ÿ**:
   - ç»Ÿä¸€ä½¿ç”¨ `MessageList` æ›¿ä»£ `List[Dict[str, Any]]`
   - ç»Ÿä¸€ä½¿ç”¨ `ToolDefinitionList` æ›¿ä»£ `Optional[List[Dict[str, Any]]]`
   - ç»Ÿä¸€ä½¿ç”¨ `ToolCall` ç±»å‹ï¼ˆç›´æ¥å¤ç”¨ OpenAI SDK ç±»å‹ï¼‰
   - åˆ é™¤é‡å¤çš„ç±»å‹å®šä¹‰ï¼ˆ`ReasoningDetail`, `ToolCallFunctionInfo`, `AccumulatedToolCall`ï¼‰

2. **ä»£ç ç»„ç»‡**:
   - åˆ é™¤ `type/decorator.py`ï¼Œè¿ç§» `HistoryList` åˆ° `type/hooks.py`
   - æ›´æ–°æ‰€æœ‰å¯¼å…¥è·¯å¾„ï¼Œä½¿ç”¨ç»Ÿä¸€çš„ç±»å‹ç³»ç»Ÿ

### ğŸ“ æ–‡æ¡£æ›´æ–°

- æ›´æ–° `llm_chat.md`: æ·»åŠ  Event Stream ä½¿ç”¨è¯´æ˜
- æ›´æ–° `llm_function.md`: æ·»åŠ  `enable_event` å‚æ•°è¯´æ˜
- æ›´æ–° `examples.md`: æ·»åŠ äº‹ä»¶æµç¤ºä¾‹è¯´æ˜

### âš ï¸ å‘åå…¼å®¹æ€§

- **å®Œå…¨å‘åå…¼å®¹**: `enable_event=False` ä¸ºé»˜è®¤å€¼ï¼Œç°æœ‰ä»£ç æ— éœ€ä¿®æ”¹
- æ‰€æœ‰ç°æœ‰ API ä¿æŒä¸å˜
- ç±»å‹ç³»ç»Ÿé‡æ„ä¸å½±å“è¿è¡Œæ—¶è¡Œä¸º

### ğŸ”® æœªæ¥è®¡åˆ’

- **v0.6.0**: `enable_event=True` å°†æˆä¸ºé»˜è®¤å€¼
- **v0.7.0**: ç§»é™¤ `enable_event` å‚æ•°ï¼Œå§‹ç»ˆå¯ç”¨äº‹ä»¶æµ

---

## 0.4.2 Release Notes

### Refactoring

1. **ReAct Engine Return Type Enhancement**: Modified `execute_llm` function to return both response and message history in streaming mode.
   - Changed return type from `AsyncGenerator[Any, None]` to `AsyncGenerator[Tuple[Any, List[Dict[str, Any]]], None]`
   - Now yields `(response, current_messages.copy())` instead of just `response`
   - Creates a copy of `current_messages` to avoid modifying the original list
   - Updated related test files to adapt to the new return type

---

## 0.4.1 Release Notes

### Features

1. **Gemini 3 Pro Preview Support**: Added `reasoning_details` field support to enable compatibility with Google Gemini 3 Pro Preview model under OpenAI-compatible interface.

2. **Reasoning Details Extraction**: 
   - Added `ReasoningDetail` type definition in `extraction.py`
   - Implemented extraction functions for both streaming and non-streaming responses
   - Support for extracting reasoning details from message objects (both dict and object formats)

3. **Message Type Enhancement**: Extended message type definitions in `message.py` to include `reasoning_details` field support.

4. **ReAct Engine Integration**: Integrated reasoning details extraction and propagation in the ReAct engine for tool call workflows.

### Examples

- Updated example files (`llm_function_pydantic_example.py`, `parallel_toolcall_example.py`, `llm_chat_raw_tooluse_example.py`) to use `gemini-3-pro-preview` model.

---

## 0.4.0 Release Notes

### Major Refactoring

1. **Modular Architecture Restructuring**: Completely refactored the base module, splitting messages, tool_call, and type_resolve into dedicated sub-modules for better code organization and maintainability.

2. **Decorator Logic Step-based Implementation**: Refactored decorator logic into a steps-based architecture within the `llm_decorator` module, improving code clarity and extensibility.

3. **Type System Enhancement**: Introduced new type support modules including decorator types and multimodal type support, expanding framework capabilities.

4. **Type Resolution System Refactoring**: Comprehensive refactoring of the type resolution system to enhance functionality support and improve type inference accuracy.

### Features

1. **Enhanced Tool Call Execution**: Improved tool call execution mechanism with extended support for multimodal interactions, enabling richer LLM interactions.

2. **Multimodal Type Support**: Added comprehensive multimodal type support throughout the framework for better handling of diverse content types.

### Bug Fixes

1. Fixed system prompt nesting issues when building multi-model content.

### Testing

Added extensive test coverage for refactored modules to ensure stability and reliability.

---

## 0.3.2.beta2 Release Notes

1. Remove dependence: `nest-asyncio`

2. Fix document error about `provider.json`

## 0.3.2.beta1 Release Notes

1. Better tool call tips in system prompt.

2. Better compound type annotations in prompt.

## 0.3.1 Release Notes

1. Added dynamic template parameter support: The `llm_function` decorator now supports passing `_template_params` to dynamically set DocString template parameters. This allows developers to create a single function that can adapt to various use cases, changing its behavior by passing different template parameters at call time.

2. Integrated Langfuse support: You can now configure `LANGFUSE_BASE_URL`, `LANGFUSE_SECRET_KEY`, and `LANGFUSE_PUBLIC_KEY` to send logs to Langfuse for tracing and analysis.

3. Added multilingual support: The English README has been updated, now supporting both Chinese and English.

4. Added parallel tool calling support.

5. Fully native async implementation: All decorators are now implemented with native async support, completely dropping any sync fallback.

## 0.2.13 Release Notes

1. Added the `return_mode` parameter (`Literal["text", "raw"]`) to the `llm_chat` decorator, allowing you to specify the return mode. You can now return either the raw response or text. This is designed to better display tool call information when developing Agents.

2. Improved code type annotations.

-----

## 0.2.12.2 Release Notes

1. Added a `py.typed` file to the framework package to support type checking.
