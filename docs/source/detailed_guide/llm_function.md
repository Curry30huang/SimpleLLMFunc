# LLM å‡½æ•°è£…é¥°å™¨

æœ¬æ–‡æ¡£ä»‹ç» SimpleLLMFunc åº“ä¸­çš„æ ¸å¿ƒè£…é¥°å™¨ `llm_function`ã€‚è¯¥è£…é¥°å™¨èƒ½å¤Ÿå°†å¼‚æ­¥ Python å‡½æ•°çš„æ‰§è¡Œå§”æ‰˜ç»™å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¼€å‘è€…åªéœ€è¦å®šä¹‰å‡½æ•°ç­¾åï¼ˆå‚æ•°å’Œè¿”å›ç±»å‹ï¼‰å¹¶åœ¨æ–‡æ¡£å­—ç¬¦ä¸²ä¸­æè¿°å‡½æ•°çš„æ‰§è¡Œç­–ç•¥ï¼ŒLLM å°±ä¼šæ ¹æ®æè¿°è‡ªåŠ¨å®Œæˆå‡½æ•°çš„å®é™…æ‰§è¡Œã€‚

## llm_function è£…é¥°å™¨

### è£…é¥°å™¨ä½œç”¨

`llm_function` è£…é¥°å™¨æ˜¯ SimpleLLMFunc åº“çš„æ ¸å¿ƒåŠŸèƒ½ä¹‹ä¸€ï¼Œå®ƒæä¾›äº†åŸç”Ÿå¼‚æ­¥çš„ LLM å‡½æ•°è°ƒç”¨èƒ½åŠ›ã€‚é€šè¿‡è¿™ä¸ªè£…é¥°å™¨ï¼Œå¼€å‘è€…å¯ä»¥è½»æ¾åœ°å°†å¼‚æ­¥å‡½æ•°è½¬æ¢ä¸ºç”± LLM æ‰§è¡Œçš„æ™ºèƒ½å‡½æ•°ã€‚

### ä¸»è¦åŠŸèƒ½ç‰¹æ€§
- **æ™ºèƒ½å‚æ•°ä¼ é€’**: è‡ªåŠ¨å°†å‡½æ•°å‚æ•°è½¬æ¢ä¸º LLM å¯ç†è§£çš„æ–‡æœ¬æç¤º
- **ç±»å‹å®‰å…¨**: æ”¯æŒç±»å‹æç¤ºï¼Œè‡ªåŠ¨å°† LLM å“åº”è½¬æ¢ä¸ºæŒ‡å®šçš„è¿”å›ç±»å‹
- **å·¥å…·é›†æˆ**: æ”¯æŒä¸º LLM æä¾›å·¥å…·ï¼Œæ‰©å±•å…¶èƒ½åŠ›èŒƒå›´
- **çµæ´»é…ç½®**: æ”¯æŒè‡ªå®šä¹‰æç¤ºæ¨¡æ¿å’Œ LLM å‚æ•°
- **é”™è¯¯å¤„ç†**: å†…ç½®é‡è¯•æœºåˆ¶å’Œè¯¦ç»†çš„æ—¥å¿—è®°å½•

## é‡è¦è¯´æ˜

> âš ï¸ `llm_function` åªèƒ½è£…é¥° `async def` å®šä¹‰çš„å¼‚æ­¥å‡½æ•°ï¼Œè¿”å›çš„ä¹Ÿæ˜¯å¯ `await` çš„åç¨‹ï¼›è¯·åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨ï¼Œæˆ–åœ¨è„šæœ¬å…¥å£ä½¿ç”¨ `asyncio.run()`ã€‚


## è£…é¥°å™¨ç”¨æ³•

### åŸºæœ¬è¯­æ³•

```python
from SimpleLLMFunc import llm_function

@llm_function(
    llm_interface=llm_interface,
    toolkit=None,
    max_tool_calls=5,
    system_prompt_template=None,
    user_prompt_template=None,
    **llm_kwargs
)
async def your_function(param1: Type1, param2: Type2) -> ReturnType:
    """åœ¨è¿™é‡Œæè¿°å‡½æ•°çš„åŠŸèƒ½å’Œæ‰§è¡Œç­–ç•¥"""
    pass
```

### å‚æ•°è¯´æ˜

- **llm_interface** (å¿…éœ€): LLM æ¥å£å®ä¾‹ï¼Œç”¨äºä¸å¤§è¯­è¨€æ¨¡å‹é€šä¿¡
- **toolkit** (å¯é€‰): å·¥å…·åˆ—è¡¨ï¼Œå¯ä»¥æ˜¯ Tool å¯¹è±¡æˆ–è¢« @tool è£…é¥°çš„å‡½æ•°
- **max_tool_calls** (å¯é€‰): æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯ï¼Œé»˜è®¤ä¸º 5
- **system_prompt_template** (å¯é€‰): è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºæ¨¡æ¿
- **user_prompt_template** (å¯é€‰): è‡ªå®šä¹‰ç”¨æˆ·æç¤ºæ¨¡æ¿
- **enable_event** (å¯é€‰): æ˜¯å¦å¯ç”¨äº‹ä»¶æµï¼Œé»˜è®¤ä¸º False
  - `False`: æ­£å¸¸æ‰§è¡Œï¼Œç›´æ¥è¿”å›è§£æåçš„ç»“æœï¼ˆå‘åå…¼å®¹æ¨¡å¼ï¼‰
  - `True`: è¿”å›ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨ï¼Œyield `ReactOutput`ï¼ˆ`ResponseYield` æˆ– `EventYield`ï¼‰
  - è¯¦ç»†è¯´æ˜è¯·å‚è€ƒ [äº‹ä»¶æµç³»ç»Ÿæ–‡æ¡£](event_stream.md)
- ****llm_kwargs**: é¢å¤–çš„å…³é”®å­—å‚æ•°ï¼Œå°†ç›´æ¥ä¼ é€’ç»™ LLM æ¥å£ï¼ˆå¦‚ temperatureã€top_p ç­‰ï¼‰

### è‡ªå®šä¹‰æç¤ºæ¨¡æ¿

#### ç³»ç»Ÿæç¤ºæ¨¡æ¿å ä½ç¬¦
- `{function_description}`: å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²å†…å®¹
- `{parameters_description}`: å‡½æ•°å‚æ•°åŠå…¶ç±»å‹çš„æè¿°
- `{return_type_description}`: è¿”å›å€¼ç±»å‹çš„æè¿°

#### ç”¨æˆ·æç¤ºæ¨¡æ¿å ä½ç¬¦
- `{parameters}`: æ ¼å¼åŒ–åçš„å‚æ•°åç§°å’Œå€¼

### åŠ¨æ€æ¨¡æ¿å‚æ•°

SimpleLLMFunc æ”¯æŒåœ¨å‡½æ•°è°ƒç”¨æ—¶é€šè¿‡ `_template_params` å‚æ•°åŠ¨æ€è®¾ç½® DocString æ¨¡æ¿å‚æ•°ã€‚è¿™ä¸ªåŠŸèƒ½è®©åŒä¸€ä¸ªå‡½æ•°å¯ä»¥é€‚åº”ä¸åŒçš„ä½¿ç”¨åœºæ™¯ï¼Œå¤§å¤§æé«˜äº†ä»£ç çš„å¤ç”¨æ€§ã€‚

#### ä½¿ç”¨æ–¹æ³•

1. **åœ¨ DocString ä¸­ä½¿ç”¨å ä½ç¬¦**ï¼š
```python
@llm_function(llm_interface=llm)
async def flexible_function(text: str) -> str:
    """ä½œä¸º{role}ï¼Œè¯·{action}ä»¥ä¸‹æ–‡æœ¬ï¼Œè¾“å‡ºé£æ ¼ä¸º{style}ã€‚"""
    pass
```

2. **è°ƒç”¨æ—¶ä¼ å…¥æ¨¡æ¿å‚æ•°**ï¼š
```python
import asyncio


async def main():
    # ç¼–è¾‘è§’è‰²
    result1 = await flexible_function(
        text,
        _template_params={
            'role': 'ä¸“ä¸šç¼–è¾‘',
            'action': 'æ¶¦è‰²',
            'style': 'å­¦æœ¯'
        }
    )

    # ç¿»è¯‘è§’è‰²
    result2 = await flexible_function(
        text,
        _template_params={
            'role': 'ç¿»è¯‘ä¸“å®¶',
            'action': 'ç¿»è¯‘',
            'style': 'å•†åŠ¡'
        }
    )

    return result1, result2


result1, result2 = asyncio.run(main())
```

#### æ ¸å¿ƒç‰¹æ€§

- **åŠ¨æ€è§’è‰²åˆ‡æ¢**ï¼šåŒä¸€ä¸ªå‡½æ•°å¯ä»¥æ‰®æ¼”ä¸åŒçš„è§’è‰²
- **çµæ´»ä»»åŠ¡é€‚é…**ï¼šæ ¹æ®è°ƒç”¨ä¸Šä¸‹æ–‡è°ƒæ•´ä»»åŠ¡ç±»å‹
- **é€æ˜å¤„ç†**ï¼š`_template_params` ä¸ä¼šä¼ é€’ç»™ LLMï¼Œä»…ç”¨äºæ¨¡æ¿å¤„ç†
- **é”™è¯¯å¤„ç†**ï¼šå½“æ¨¡æ¿å‚æ•°ä¸å®Œæ•´æ—¶ï¼Œç³»ç»Ÿä¼šå‘å‡ºè­¦å‘Šå¹¶ä½¿ç”¨åŸå§‹ DocString

## è£…é¥°å™¨è¡Œä¸º

### æ•°æ®æµç¨‹

1. **å‡½æ•°è°ƒç”¨æ•è·**: å½“ç”¨æˆ·è°ƒç”¨è¢«è£…é¥°çš„å‡½æ•°æ—¶ï¼Œè£…é¥°å™¨æ•è·æ‰€æœ‰å®é™…å‚æ•°
2. **ç±»å‹ä¿¡æ¯æå–**: ä»å‡½æ•°ç­¾åä¸­æå–å‚æ•°ç±»å‹å’Œè¿”å›ç±»å‹ä¿¡æ¯
3. **æç¤ºæ„å»º**: 
   - å°†å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²ä½œä¸ºç³»ç»Ÿæç¤ºçš„æ ¸å¿ƒ
   - å°†å‚æ•°ä¿¡æ¯æ ¼å¼åŒ–ä¸ºç”¨æˆ·æç¤º
   - åº”ç”¨è‡ªå®šä¹‰æ¨¡æ¿ï¼ˆå¦‚æœæä¾›ï¼‰
4. **LLM è°ƒç”¨**: å‘é€æ„å»ºå¥½çš„æç¤ºç»™ LLM
5. **å·¥å…·å¤„ç†**: å¦‚æœ LLM éœ€è¦ä½¿ç”¨å·¥å…·ï¼Œè‡ªåŠ¨å¤„ç†å·¥å…·è°ƒç”¨
6. **å“åº”è½¬æ¢**: å°† LLM çš„æ–‡æœ¬å“åº”è½¬æ¢ä¸ºæŒ‡å®šçš„è¿”å›ç±»å‹
7. **ç»“æœè¿”å›**: è¿”å›è½¬æ¢åçš„ç»“æœç»™è°ƒç”¨è€…

### å†…ç½®åŠŸèƒ½

#### ç±»å‹è½¬æ¢æ”¯æŒ
- åŸºæœ¬ç±»å‹ï¼š`str`, `int`, `float`, `bool`
- å®¹å™¨ç±»å‹ï¼š`List`, `Dict`, `Tuple`
- Pydantic æ¨¡å‹
- è‡ªå®šä¹‰ç±»å‹ï¼ˆé€šè¿‡ JSON åºåˆ—åŒ–ï¼‰

#### é”™è¯¯å¤„ç†æœºåˆ¶
- **ç©ºå“åº”é‡è¯•**: å½“ LLM è¿”å›ç©ºå†…å®¹æ—¶è‡ªåŠ¨é‡è¯•
- **å¼‚å¸¸æ•è·**: å®Œæ•´çš„å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—è®°å½•
- **ç±»å‹è½¬æ¢é”™è¯¯**: ä¼˜é›…å¤„ç†ç±»å‹è½¬æ¢å¤±è´¥çš„æƒ…å†µ

#### æ—¥å¿—è®°å½•
- è¯¦ç»†çš„æ‰§è¡Œæ—¥å¿—ï¼ŒåŒ…æ‹¬å‚æ•°ã€æç¤ºå†…å®¹å’Œå“åº”
- æ”¯æŒè¿½è¸ª IDï¼Œä¾¿äºè°ƒè¯•å¤æ‚çš„è°ƒç”¨é“¾
- åˆ†çº§æ—¥å¿—ï¼ˆè°ƒè¯•ã€ä¿¡æ¯ã€è­¦å‘Šã€é”™è¯¯ï¼‰

## ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºæœ¬æ–‡æœ¬å¤„ç†

```python
import asyncio
from SimpleLLMFunc import llm_function, OpenAICompatible

# åˆå§‹åŒ– LLM æ¥å£ï¼ˆæ¨èæ–¹å¼ï¼šä»é…ç½®æ–‡ä»¶åŠ è½½ï¼‰
models = OpenAICompatible.load_from_json_file("provider.json")
llm = models["openai"]["gpt-3.5-turbo"]

@llm_function(llm_interface=llm)
async def summarize_text(text: str, max_words: int = 100) -> str:
    """æ ¹æ®è¾“å…¥æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œæ‘˜è¦ä¸è¶…è¿‡æŒ‡å®šçš„è¯æ•°ã€‚"""
    pass

# ä½¿ç”¨å‡½æ•°
long_text = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬..."


async def main():
    summary = await summarize_text(long_text, max_words=50)
    print(summary)


asyncio.run(main())
```

### ç¤ºä¾‹ 2: ç»“æ„åŒ–æ•°æ®è¿”å›

```python
import asyncio
from typing import Dict, Any, List

@llm_function(llm_interface=llm)
async def analyze_sentiment(text: str) -> Dict[str, Any]:
    """
    åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼Œè¿”å›åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
    - sentiment: æƒ…æ„Ÿæ ‡ç­¾ï¼ˆpositive/negative/neutralï¼‰
    - confidence: ç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰
    - keywords: å…³é”®è¯åˆ—è¡¨
    """
    pass

# ä½¿ç”¨å‡½æ•°


async def main():
    result = await analyze_sentiment("æˆ‘ä»Šå¤©å¿ƒæƒ…å¾ˆå¥½ï¼Œå¤©æ°”ä¹Ÿå¾ˆæ£’ï¼")
    print(result)


asyncio.run(main())
# è¾“å‡º: {'sentiment': 'positive', 'confidence': 0.95, 'keywords': ['å¿ƒæƒ…', 'å¥½', 'å¤©æ°”', 'æ£’']}
```

### ç¤ºä¾‹ 3: ä½¿ç”¨å·¥å…·é›†

```python
import asyncio
from SimpleLLMFunc import tool, llm_function, OpenAICompatible

# åŠ è½½æ¨¡å‹æ¥å£
models = OpenAICompatible.load_from_json_file("provider.json")
llm = models["openai"]["gpt-3.5-turbo"]

@tool
async def search_web(query: str) -> str:
    """åœ¨ç½‘ç»œä¸Šæœç´¢ä¿¡æ¯"""
    # å®ç°ç½‘ç»œæœç´¢é€»è¾‘
    return f"æœç´¢ç»“æœ: {query}"

@tool
async def calculate(expression: str) -> float:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    return eval(expression)

@llm_function(
    llm_interface=llm,
    toolkit=[search_web, calculate]
)
async def research_and_calculate(topic: str, calculation: str) -> str:
    """
    æ ¹æ®ä¸»é¢˜æœç´¢ç›¸å…³ä¿¡æ¯ï¼Œå¹¶è¿›è¡ŒæŒ‡å®šçš„è®¡ç®—ï¼Œ
    æœ€åæ•´åˆä¿¡æ¯ç»™å‡ºç»¼åˆæŠ¥å‘Šã€‚
    """
    pass

# ä½¿ç”¨å‡½æ•°
async def main():
    result = await research_and_calculate(
        topic="Pythonç¼–ç¨‹è¯­è¨€",
        calculation="2023 - 1991"
    )
    print(result)


asyncio.run(main())
```

### ç¤ºä¾‹ 4: è‡ªå®šä¹‰æç¤ºæ¨¡æ¿

```python
import asyncio

# è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºæ¨¡æ¿
custom_system_template = """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æ‰§è¡Œä»»åŠ¡:

å‚æ•°ä¿¡æ¯:
{parameters_description}

è¿”å›ç±»å‹: {return_type_description}

ä»»åŠ¡æè¿°:
{function_description}

è¯·ç¡®ä¿åˆ†æç»“æœå‡†ç¡®ã€å®¢è§‚ï¼Œå¹¶æä¾›æ•°æ®æ”¯æŒã€‚
"""

# è‡ªå®šä¹‰ç”¨æˆ·æç¤ºæ¨¡æ¿
custom_user_template = """
è¯·åˆ†æä»¥ä¸‹æ•°æ®:
{parameters}

è¯·ç›´æ¥æä¾›åˆ†æç»“æœï¼Œæ ¼å¼è¦æ±‚ä¸ºæŒ‡å®šçš„è¿”å›ç±»å‹ã€‚
"""

@llm_function(
    llm_interface=llm,
    system_prompt_template=custom_system_template,
    user_prompt_template=custom_user_template,
    temperature=0.7,  # é€šè¿‡ llm_kwargs ä¼ é€’æ¨¡å‹å‚æ•°
    top_p=0.9
)
async def analyze_data(data: List[Dict[str, Any]], analysis_type: str) -> Dict[str, Any]:
    """
    å¯¹ç»™å®šçš„æ•°æ®é›†è¿›è¡ŒæŒ‡å®šç±»å‹çš„åˆ†æï¼Œ
    æ”¯æŒçš„åˆ†æç±»å‹åŒ…æ‹¬ï¼šè¶‹åŠ¿åˆ†æã€å¼‚å¸¸æ£€æµ‹ã€ç»Ÿè®¡æ‘˜è¦ç­‰ã€‚
    """
    pass

# ä½¿ç”¨å‡½æ•°
sample_data = [
    {"date": "2023-01-01", "value": 100},
    {"date": "2023-01-02", "value": 120},
    {"date": "2023-01-03", "value": 95}
]

async def main():
    analysis_result = await analyze_data(sample_data, "è¶‹åŠ¿åˆ†æ")
    print(analysis_result)


asyncio.run(main())
```

### ç¤ºä¾‹ 5: Pydantic æ¨¡å‹è¿”å›

```python
import asyncio
from pydantic import BaseModel
from typing import List

class TaskResult(BaseModel):
    success: bool
    message: str
    tasks: List[str]
    estimated_time: int

@llm_function(llm_interface=llm)
async def create_project_plan(project_description: str, deadline_days: int) -> TaskResult:
    """
    æ ¹æ®é¡¹ç›®æè¿°å’Œæˆªæ­¢æ—¶é—´ï¼Œåˆ¶å®šè¯¦ç»†çš„é¡¹ç›®è®¡åˆ’ã€‚
    è¿”å›åŒ…å«ä»»åŠ¡åˆ—è¡¨ã€é¢„ä¼°æ—¶é—´å’Œæ‰§è¡Œå»ºè®®çš„ç»“æ„åŒ–ç»“æœã€‚
    """
    pass

# ä½¿ç”¨å‡½æ•°


async def main():
    plan = await create_project_plan(
        project_description="å¼€å‘ä¸€ä¸ªç®€å•çš„å¾…åŠäº‹é¡¹åº”ç”¨",
        deadline_days=30
    )

    print(f"è®¡åˆ’åˆ¶å®šæˆåŠŸ: {plan.success}")
    print(f"å»ºè®®: {plan.message}")
    print(f"ä»»åŠ¡åˆ—è¡¨: {plan.tasks}")
    print(f"é¢„ä¼°æ—¶é—´: {plan.estimated_time}å¤©")


asyncio.run(main())
```

### ç¤ºä¾‹ 6: åŠ¨æ€æ¨¡æ¿å‚æ•°

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨åŠ¨æ€æ¨¡æ¿å‚æ•°è®©åŒä¸€ä¸ªå‡½æ•°é€‚åº”ä¸åŒçš„ä½¿ç”¨åœºæ™¯ï¼š

```python
import asyncio


@llm_function(llm_interface=llm)
async def analyze_code(code: str) -> str:
    """ä»¥{style}çš„æ–¹å¼åˆ†æ{language}ä»£ç ï¼Œé‡ç‚¹å…³æ³¨{focus}ã€‚"""
    pass


@llm_function(llm_interface=llm)
async def process_text(text: str) -> str:
    """ä½œä¸º{role}ï¼Œè¯·{action}ä»¥ä¸‹æ–‡æœ¬ï¼Œè¾“å‡ºé£æ ¼ä¸º{style}ã€‚"""
    pass


# ä½¿ç”¨ç¤ºä¾‹
python_code = """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
"""


async def main():
    # ä¸åŒçš„åˆ†ææ–¹å¼
    performance_analysis = await analyze_code(
        python_code,
        _template_params={
            'style': 'è¯¦ç»†',
            'language': 'Python',
            'focus': 'æ€§èƒ½ä¼˜åŒ–'
        }
    )

    code_style_analysis = await analyze_code(
        python_code,
        _template_params={
            'style': 'ç®€æ´',
            'language': 'Python',
            'focus': 'ä»£ç è§„èŒƒ'
        }
    )

    # ä¸åŒçš„æ–‡æœ¬å¤„ç†è§’è‰²
    sample_text = "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œå¯¹å„è¡Œå„ä¸šäº§ç”Ÿæ·±è¿œå½±å“ã€‚"

    edited_text = await process_text(
        sample_text,
        _template_params={
            'role': 'ä¸“ä¸šç¼–è¾‘',
            'action': 'æ¶¦è‰²',
            'style': 'å­¦æœ¯'
        }
    )

    translated_text = await process_text(
        sample_text,
        _template_params={
            'role': 'ç¿»è¯‘ä¸“å®¶',
            'action': 'ç¿»è¯‘æˆè‹±æ–‡',
            'style': 'å•†åŠ¡'
        }
    )

    print("æ€§èƒ½åˆ†æç»“æœ:", performance_analysis)
    print("ä»£ç è§„èŒƒåˆ†æ:", code_style_analysis)
    print("ç¼–è¾‘æ¶¦è‰²ç»“æœ:", edited_text)
    print("ç¿»è¯‘ç»“æœ:", translated_text)


asyncio.run(main())
```

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†åŠ¨æ€æ¨¡æ¿å‚æ•°çš„å¼ºå¤§åŠŸèƒ½ï¼š
- **ä¸€ä¸ªå‡½æ•°ï¼Œå¤šç§åœºæ™¯**ï¼š`analyze_code` å¯ä»¥ç”¨äºæ€§èƒ½åˆ†æã€è§„èŒƒæ£€æŸ¥ç­‰ä¸åŒç›®çš„
- **åŠ¨æ€è§’è‰²åˆ‡æ¢**ï¼š`process_text` å¯ä»¥æ‰®æ¼”ç¼–è¾‘ã€ç¿»è¯‘ç­‰ä¸åŒè§’è‰²
- **çµæ´»ä»»åŠ¡é€‚é…**ï¼šæ ¹æ®è°ƒç”¨æ—¶çš„å‚æ•°åŠ¨æ€è°ƒæ•´ä»»åŠ¡ç±»å‹å’Œè¾“å‡ºé£æ ¼

---

é€šè¿‡è¿™äº›ç¤ºä¾‹å¯ä»¥çœ‹å‡ºï¼Œ`llm_function` è£…é¥°å™¨æä¾›äº†ä¸€ç§ç®€æ´è€Œå¼ºå¤§çš„æ–¹å¼æ¥åˆ©ç”¨ LLM çš„èƒ½åŠ›ï¼ŒåŒæ—¶ä¿æŒäº† Python ä»£ç çš„ç±»å‹å®‰å…¨æ€§å’Œå¯è¯»æ€§ã€‚

## å¼‚æ­¥ä½¿ç”¨ç¤ºä¾‹

`llm_function` è‡ªèº«å³ä¸ºåŸç”Ÿå¼‚æ­¥å®ç°ã€‚ä»¥ä¸‹ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•åœ¨ä¸åŒåœºæ™¯ä¸‹ä½¿ç”¨å®ƒï¼š

### ç¤ºä¾‹ 1: åŸºæœ¬å¼‚æ­¥ç”¨æ³•

```python
import asyncio
from SimpleLLMFunc import llm_function, OpenAICompatible

# ä»é…ç½®æ–‡ä»¶åŠ è½½
models = OpenAICompatible.load_from_json_file("provider.json")
llm = models["openai"]["gpt-3.5-turbo"]


@llm_function(llm_interface=llm)
async def summarize_text_async(text: str, max_words: int = 100) -> str:
    """æ ¹æ®è¾“å…¥æ–‡æœ¬ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œæ‘˜è¦ä¸è¶…è¿‡æŒ‡å®šçš„è¯æ•°ã€‚"""
    pass


async def main():
    long_text = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬..."
    summary = await summarize_text_async(long_text, max_words=50)
    print(summary)


asyncio.run(main())
```

### ç¤ºä¾‹ 2: å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚

```python
import asyncio


@llm_function(llm_interface=llm)
async def translate_text_async(text: str, target_language: str = "English") -> str:
    """å°†è¾“å…¥æ–‡æœ¬ç¿»è¯‘æˆæŒ‡å®šè¯­è¨€ã€‚"""
    pass


@llm_function(llm_interface=llm)
async def analyze_sentiment_async(text: str) -> str:
    """åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ã€‚"""
    pass


async def process_texts_concurrently():
    texts = [
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
        "æˆ‘æ„Ÿåˆ°å¾ˆæ²®ä¸§",
        "è¿™ä¸ªäº§å“è´¨é‡å¾ˆæ£’"
    ]

    translation_tasks = [translate_text_async(text) for text in texts]
    sentiment_tasks = [analyze_sentiment_async(text) for text in texts]

    translations, sentiments = await asyncio.gather(
        asyncio.gather(*translation_tasks),
        asyncio.gather(*sentiment_tasks)
    )

    for i, (text, translation, sentiment) in enumerate(zip(texts, translations, sentiments)):
        print(f"æ–‡æœ¬ {i + 1}: {text}")
        print(f"ç¿»è¯‘: {translation}")
        print(f"æƒ…æ„Ÿ: {sentiment}")


asyncio.run(process_texts_concurrently())
```

### ç¤ºä¾‹ 3: ä¸å…¶ä»–å¼‚æ­¥æ“ä½œé…åˆ

```python
import aiohttp
import asyncio


@llm_function(llm_interface=llm)
async def process_content_async(content: str) -> str:
    """å¤„ç†ä»ç½‘ç»œè·å–çš„å†…å®¹ã€‚"""
    pass


async def fetch_and_process_url(session: aiohttp.ClientSession, url: str) -> str:
    async with session.get(url) as response:
        content = await response.text()

    processed = await process_content_async(content[:1000])
    return processed


async def process_multiple_urls():
    urls = [
        "https://example1.com",
        "https://example2.com",
        "https://example3.com"
    ]

    async with aiohttp.ClientSession() as session:
        tasks = [fetch_and_process_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for url, result in zip(urls, results):
            if isinstance(result, Exception):
                print(f"å¤„ç† {url} æ—¶å‡ºé”™: {result}")
            else:
                print(f"{url}: {result}")


asyncio.run(process_multiple_urls())
```

è¿™äº›ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `llm_function` åœ¨å¼‚æ­¥ç¯å¢ƒä¸­æ„å»ºé«˜å¹¶å‘çš„ LLM è°ƒç”¨é€»è¾‘ã€‚

## äº‹ä»¶æµä½¿ç”¨

`llm_function` æ”¯æŒäº‹ä»¶æµï¼Œå…è®¸ä½ å®æ—¶è§‚å¯Ÿå‡½æ•°æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ LLM è°ƒç”¨ã€Token ç”¨é‡ç­‰ä¿¡æ¯ã€‚è¿™å¯¹äºæ€§èƒ½ç›‘æ§ã€æˆæœ¬è¿½è¸ªå’Œè°ƒè¯•éå¸¸æœ‰ç”¨ã€‚

### åŸºæœ¬ç”¨æ³•

```python
from SimpleLLMFunc import llm_function
from SimpleLLMFunc.hooks import ResponseYield, EventYield
from SimpleLLMFunc.hooks.events import LLMCallEndEvent

@llm_function(
    llm_interface=llm,
    enable_event=True,  # ğŸ”‘ å¯ç”¨äº‹ä»¶æµ
)
async def analyze_text(text: str) -> str:
    """åˆ†ææ–‡æœ¬å¹¶æä¾›è§è§£"""
    pass

# å¤„ç†äº‹ä»¶å’Œå“åº”
async for output in analyze_text("Sample text"):
    if isinstance(output, ResponseYield):
        # è·å–æœ€ç»ˆç»“æœï¼ˆå·²è§£æä¸ºæŒ‡å®šçš„è¿”å›ç±»å‹ï¼‰
        print(f"åˆ†æç»“æœ: {output.response}")
    elif isinstance(output, EventYield):
        event = output.event
        # å¤„ç†å„ç§äº‹ä»¶
        if isinstance(event, LLMCallEndEvent):
            usage = event.usage
            if usage:
                print(f"Token ç”¨é‡: {usage.total_tokens}")
```

### Token ç”¨é‡ç›‘æ§ç¤ºä¾‹

å‚è€ƒå®Œæ•´ç¤ºä¾‹ï¼š[examples/llm_function_token_usage.py](https://github.com/NiJingzhe/SimpleLLMFunc/blob/master/examples/llm_function_token_usage.py)

```python
total_tokens = 0

async for output in summarize_text(text="é•¿æ–‡æœ¬..."):
    if isinstance(output, EventYield):
        event = output.event
        if isinstance(event, LLMCallEndEvent) and event.usage:
            total_tokens += event.usage.total_tokens
    elif isinstance(output, ResponseYield):
        print(f"æ‘˜è¦: {output.response}")

print(f"æ€»è®¡ä½¿ç”¨ Token: {total_tokens}")
```

**è¯¦ç»†æ–‡æ¡£**ï¼šè¯·å‚è€ƒ [äº‹ä»¶æµç³»ç»Ÿæ–‡æ¡£](event_stream.md) äº†è§£å®Œæ•´çš„äº‹ä»¶ç±»å‹ã€ä½¿ç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µã€‚

## æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†

```python
async def robust_llm_call():
    try:
        result = await your_llm_function("input")
        return result
    except Exception as e:
        print(f"LLM è°ƒç”¨å¤±è´¥: {e}")
        return "é»˜è®¤å€¼"
```

### 2. è¶…æ—¶æ§åˆ¶

```python
async def llm_call_with_timeout():
    try:
        result = await asyncio.wait_for(
            your_llm_function("input"),
            timeout=30.0  # 30ç§’è¶…æ—¶
        )
        return result
    except asyncio.TimeoutError:
        print("LLM è°ƒç”¨è¶…æ—¶")
        return "è¶…æ—¶é»˜è®¤å€¼"
```

---

é€šè¿‡è¿™äº›ç¤ºä¾‹å¯ä»¥çœ‹å‡ºï¼Œ`llm_function` è£…é¥°å™¨åœ¨å¼‚æ­¥åœºæ™¯ä¸‹åŒæ ·èƒ½å¤Ÿæä¾›é«˜æ€§èƒ½çš„ LLM è°ƒç”¨èƒ½åŠ›ï¼Œå¹¶ä¿æŒäº†è‰¯å¥½çš„æ˜“ç”¨æ€§ä¸åŠŸèƒ½å®Œæ•´æ€§ã€‚
