"""
äº‹ä»¶æµ Chatbot ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨ SimpleLLMFunc çš„äº‹ä»¶æµåŠŸèƒ½æ„å»ºä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„èŠå¤©æœºå™¨äººã€‚
ä½¿ç”¨ rich åº“å®ç°ç¾è§‚çš„ç»ˆç«¯ UIï¼Œå®æ—¶æ˜¾ç¤ºï¼š
- LLM æµå¼å“åº”ï¼ˆMarkdown æ¸²æŸ“ï¼‰
- å·¥å…·è°ƒç”¨è¿‡ç¨‹ï¼ˆå‚æ•°å’Œç»“æœï¼‰
- æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯ï¼ˆtoken ä½¿ç”¨ã€è€—æ—¶ç­‰ï¼‰

è¿è¡Œè¦æ±‚ï¼š
    pip install rich

ä½¿ç”¨æ–¹æ³•ï¼š
    python event_stream_chatbot.py

åŠŸèƒ½ç‰¹æ€§ï¼š
- å®æ—¶æµå¼å“åº”æ¸²æŸ“
- å·¥å…·è°ƒç”¨å¯è§†åŒ–
- äº‹ä»¶é©±åŠ¨çš„ UI æ›´æ–°
- å®Œæ•´çš„æ‰§è¡Œç»Ÿè®¡
"""

import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime

from rich.console import Console
from rich.table import Table

from SimpleLLMFunc import llm_chat, tool
from SimpleLLMFunc.interface.openai_compatible import OpenAICompatible
from SimpleLLMFunc.interface.llm_interface import LLM_Interface
from SimpleLLMFunc.hooks import (
    ReactOutput,
    ResponseYield,
    EventYield,
    ReActEventType,
    ReactStartEvent,
    LLMCallStartEvent,
    LLMChunkArriveEvent,
    LLMCallEndEvent,
    ToolCallStartEvent,
    ToolCallEndEvent,
    ToolCallsBatchStartEvent,
    ToolCallsBatchEndEvent,
    ReactEndEvent,
)

# åˆå§‹åŒ– Rich Console
console = Console()


# ============================================================================
# å·¥å…·å®šä¹‰
# ============================================================================

@tool(name="calculate", description="æ‰§è¡Œæ•°å­¦è®¡ç®—ï¼Œæ”¯æŒåŸºæœ¬çš„ç®—æœ¯è¿ç®—")
async def calculate(expression: str) -> str:
    """
    æ‰§è¡Œæ•°å­¦è®¡ç®—ã€‚
    
    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼ï¼Œä¾‹å¦‚ "2 + 3 * 4"
    
    Returns:
        è®¡ç®—ç»“æœ
    """
    try:
        # å®‰å…¨çš„æ•°å­¦è®¡ç®—ï¼ˆä»…æ”¯æŒåŸºæœ¬è¿ç®—ï¼‰
        result = eval(expression, {"__builtins__": {}}, {})
        return f"è®¡ç®—ç»“æœ: {expression} = {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯: {str(e)}"


@tool(name="get_weather", description="æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯")
async def get_weather(city: str) -> str:
    """
    æŸ¥è¯¢å¤©æ°”ä¿¡æ¯ï¼ˆæ¨¡æ‹Ÿï¼‰ã€‚
    
    Args:
        city: åŸå¸‚åç§°
    
    Returns:
        å¤©æ°”ä¿¡æ¯
    """
    # æ¨¡æ‹Ÿå¼‚æ­¥ API è°ƒç”¨
    await asyncio.sleep(0.5)
    
    # æ¨¡æ‹Ÿå¤©æ°”æ•°æ®
    weather_data = {
        "åŒ—äº¬": "æ™´å¤©ï¼Œæ¸©åº¦ 15-25Â°Cï¼Œç©ºæ°”è´¨é‡è‰¯å¥½",
        "ä¸Šæµ·": "å¤šäº‘ï¼Œæ¸©åº¦ 18-28Â°Cï¼Œæœ‰è½»å¾®é›¾éœ¾",
        "æ·±åœ³": "é˜´å¤©ï¼Œæ¸©åº¦ 22-30Â°Cï¼Œå¯èƒ½æœ‰å°é›¨",
    }
    
    return weather_data.get(city, f"{city}çš„å¤©æ°”ä¿¡æ¯ï¼šæ™´å¤©ï¼Œæ¸©åº¦é€‚ä¸­")


@tool(name="search_knowledge", description="æœç´¢çŸ¥è¯†åº“è·å–ä¸“ä¸šä¿¡æ¯")
async def search_knowledge(query: str) -> str:
    """
    æœç´¢çŸ¥è¯†åº“ï¼ˆæ¨¡æ‹Ÿï¼‰ã€‚
    
    Args:
        query: æœç´¢æŸ¥è¯¢
    
    Returns:
        æœç´¢ç»“æœ
    """
    await asyncio.sleep(0.3)
    
    # æ¨¡æ‹ŸçŸ¥è¯†åº“æœç´¢
    knowledge = {
        "python": "Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åº“ç”Ÿæ€ç³»ç»Ÿè€Œé—»åã€‚",
        "ai": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
        "llm": "å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡åœ¨å¤§é‡æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚",
    }
    
    for key, value in knowledge.items():
        if key in query.lower():
            return f"çŸ¥è¯†åº“ç»“æœï¼š{value}"
    
    return f"å…³äº '{query}' çš„ä¿¡æ¯ï¼šè¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„è¯é¢˜ï¼Œå»ºè®®è¿›ä¸€æ­¥æ¢ç´¢ã€‚"


# ============================================================================
# äº‹ä»¶å¤„ç†å’Œ UI æ¸²æŸ“
# ============================================================================

class ChatbotUI:
    """èŠå¤©æœºå™¨äºº UI ç®¡ç†å™¨"""
    
    def __init__(self):
        self.console = console
        self.current_response = ""
        self.tool_calls_info: List[Dict[str, Any]] = []
        self.stats = {
            "total_llm_calls": 0,
            "total_tool_calls": 0,
            "total_chunks": 0,
            "start_time": None,
            "end_time": None,
        }
    
    def print_user_message(self, message: str):
        """æ‰“å°ç”¨æˆ·æ¶ˆæ¯"""
        self.console.print()
        self.console.print("[bold cyan]ğŸ‘¤ ç”¨æˆ·[/bold cyan]")
        self.console.print(f"[cyan]{message}[/cyan]")
        self.console.print()
    
    def print_system_info(self, message: str, style: str = "dim"):
        """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
        self.console.print(f"[{style}]â„¹ï¸  {message}[/{style}]")
    
    def handle_react_start(self, event: ReactStartEvent):
        """å¤„ç† ReAct å¼€å§‹äº‹ä»¶"""
        self.stats["start_time"] = event.timestamp
        self.console.print()
        self.print_system_info(
            f"ğŸš€ å¼€å§‹å¤„ç†è¯·æ±‚ (trace_id: {event.trace_id[:8]}...)",
            "bold green"
        )
    
    def handle_llm_call_start(self, event: LLMCallStartEvent):
        """å¤„ç† LLM è°ƒç”¨å¼€å§‹äº‹ä»¶"""
        self.stats["total_llm_calls"] += 1
        self.current_response = ""
        
        # æ˜¾ç¤º LLM è°ƒç”¨ä¿¡æ¯
        tools_info = f", {len(event.tools)} å·¥å…·å¯ç”¨" if event.tools else ", æ— å·¥å…·"
        mode_info = "æµå¼" if event.stream else "éæµå¼"
        
        self.print_system_info(
            f"ğŸ¤– LLM è°ƒç”¨ #{self.stats['total_llm_calls']} ({mode_info}{tools_info})"
        )
    
    def handle_llm_chunk(self, event: LLMChunkArriveEvent):
        """å¤„ç† LLM æµå¼å—äº‹ä»¶"""
        self.stats["total_chunks"] += 1
        # ç´¯ç§¯å“åº”å†…å®¹
        self.current_response = event.accumulated_content
        
        # é¦–æ¬¡ chunk æ—¶æ‰“å°æ ‡é¢˜å’Œä¸Šåˆ†éš”çº¿
        if event.chunk_index == 0:
            self.console.print()
            self.console.print("[bold green]ğŸ¤– åŠ©æ‰‹å›å¤[/bold green]")
            # æ‰“å°æ¨ªçº¿åˆ†éš”ç¬¦
            self.console.print("â”€" * 80)
        
        # ä» chunk ä¸­æå–å¢é‡å†…å®¹å¹¶æ‰“å°
        from SimpleLLMFunc.base.post_process import extract_content_from_stream_response
        chunk_content = extract_content_from_stream_response(event.chunk, "chatbot")
        if chunk_content:
            self.render_response_chunk(chunk_content)
    
    def handle_llm_call_end(self, event: LLMCallEndEvent):
        """å¤„ç† LLM è°ƒç”¨ç»“æŸäº‹ä»¶"""
        # æ‰“å°æ¢è¡Œç¬¦ï¼ˆæœ€åä¸€ä¸ª token åé¢ï¼‰
        self.console.print()
        # æ‰“å°ä¸‹åˆ†éš”çº¿
        self.console.print("â”€" * 80)
        
        # æ˜¾ç¤º token ä½¿ç”¨æƒ…å†µï¼ˆç°åœ¨ usage æ˜¯ CompletionUsage å¯¹è±¡ï¼Œæœ‰è‰¯å¥½çš„ä»£ç è¡¥å…¨ï¼‰
        if event.usage:
            self.print_system_info(
                f"ğŸ“Š Token ä½¿ç”¨: "
                f"è¾“å…¥={event.usage.prompt_tokens}, "
                f"è¾“å‡º={event.usage.completion_tokens}, "
                f"æ€»è®¡={event.usage.total_tokens}"
            )
    
    def handle_tool_calls_batch_start(self, event: ToolCallsBatchStartEvent):
        """å¤„ç†å·¥å…·è°ƒç”¨æ‰¹æ¬¡å¼€å§‹äº‹ä»¶"""
        self.console.print()
        self.print_system_info(
            f"ğŸ”§ å¼€å§‹æ‰§è¡Œ {event.batch_size} ä¸ªå·¥å…·è°ƒç”¨",
            "bold yellow"
        )
        self.tool_calls_info = []
    
    def handle_tool_call_start(self, event: ToolCallStartEvent):
        """å¤„ç†å•ä¸ªå·¥å…·è°ƒç”¨å¼€å§‹äº‹ä»¶"""
        self.stats["total_tool_calls"] += 1
        
        # åˆ›å»ºå·¥å…·è°ƒç”¨ä¿¡æ¯è¡¨æ ¼
        table = Table(show_header=True, header_style="bold magenta", box=None)
        table.add_column("å‚æ•°", style="cyan")
        table.add_column("å€¼", style="white")
        
        for key, value in event.arguments.items():
            table.add_row(key, str(value))
        
        self.console.print()
        self.console.print(f"[bold yellow]ğŸ› ï¸  å·¥å…·è°ƒç”¨: {event.tool_name}[/bold yellow]")
        self.console.print(table)
    
    def handle_tool_call_end(self, event: ToolCallEndEvent):
        """å¤„ç†å·¥å…·è°ƒç”¨ç»“æŸäº‹ä»¶"""
        # æ˜¾ç¤ºå·¥å…·æ‰§è¡Œç»“æœ
        result_text = str(event.result)[:200]  # é™åˆ¶é•¿åº¦
        if len(str(event.result)) > 200:
            result_text += "..."
        
        status = "âœ… æˆåŠŸ" if event.success else "âŒ å¤±è´¥"
        self.console.print(
            f"  {status} ({event.execution_time:.2f}s): [dim]{result_text}[/dim]"
        )
    
    def handle_tool_calls_batch_end(self, event: ToolCallsBatchEndEvent):
        """å¤„ç†å·¥å…·è°ƒç”¨æ‰¹æ¬¡ç»“æŸäº‹ä»¶"""
        success_count = sum(1 for r in event.tool_results if r["success"])
        self.print_system_info(
            f"âœ¨ å·¥å…·è°ƒç”¨å®Œæˆ: {success_count}/{len(event.tool_results)} æˆåŠŸ, "
            f"æ€»è€—æ—¶ {event.total_execution_time:.2f}s"
        )
    
    def handle_react_end(self, event: ReactEndEvent):
        """å¤„ç† ReAct ç»“æŸäº‹ä»¶"""
        self.stats["end_time"] = event.timestamp
        
        # è®¡ç®—æ€»è€—æ—¶
        if self.stats["start_time"] and self.stats["end_time"]:
            duration = (self.stats["end_time"] - self.stats["start_time"]).total_seconds()
        else:
            duration = 0
        
        # æ˜¾ç¤ºå®Œæ•´ç»Ÿè®¡ä¿¡æ¯
        self.console.print()
        stats_table = Table(show_header=False, box=None, padding=(0, 2))
        stats_table.add_column("é¡¹ç›®", style="bold")
        stats_table.add_column("å€¼", style="cyan")
        
        stats_table.add_row("æ€»è€—æ—¶", f"{duration:.2f}s")
        stats_table.add_row("LLM è°ƒç”¨æ¬¡æ•°", str(event.total_llm_calls))
        stats_table.add_row("å·¥å…·è°ƒç”¨æ¬¡æ•°", str(event.total_tool_calls))
        stats_table.add_row("æµå¼å—æ•°é‡", str(self.stats["total_chunks"]))
        
        if event.total_token_usage:
            # total_token_usage æ˜¯ CompletionUsage å¯¹è±¡ï¼Œæœ‰è‰¯å¥½çš„ä»£ç è¡¥å…¨
            stats_table.add_row(
                "æ€» Token ä½¿ç”¨",
                f"{event.total_token_usage.total_tokens} "
                f"(è¾“å…¥: {event.total_token_usage.prompt_tokens}, "
                f"è¾“å‡º: {event.total_token_usage.completion_tokens})"
            )
        
        self.console.print()
        self.console.print("[bold green]ğŸ“ˆ æ‰§è¡Œç»Ÿè®¡[/bold green]")
        self.console.print(stats_table)
    
    def render_response_chunk(self, text: str):
        """æ¸²æŸ“å“åº”æ–‡æœ¬å—ï¼ˆæµå¼ï¼‰"""
        # ç›´æ¥æ‰“å°æ–‡æœ¬å—ï¼Œä¸æ¢è¡Œ
        self.console.print(text, end="", style="bold white")


# ============================================================================
# äº‹ä»¶æµå¤„ç†åŒ…è£…å™¨
# ============================================================================

async def chatbot_with_event_ui(
    message: str,
    history: Optional[List[Dict[str, str]]] = None,
    llm_interface: Optional[LLM_Interface] = None,
) -> tuple[str, List[Dict[str, str]]]:
    """
    å¸¦æœ‰äº‹ä»¶æµ UI çš„èŠå¤©æœºå™¨äººåŒ…è£…å™¨ã€‚
    
    è¿™ä¸ªå‡½æ•°å±•ç¤ºå¦‚ä½•åœ¨å¤–éƒ¨å¤„ç†äº‹ä»¶æµï¼Œå®ç°è‡ªå®šä¹‰çš„ UI å’Œé€»è¾‘ã€‚
    
    Args:
        message: ç”¨æˆ·æ¶ˆæ¯
        history: å¯¹è¯å†å²
        llm_interface: LLM æ¥å£å®ä¾‹
        
    Returns:
        (æœ€ç»ˆå“åº”, æ›´æ–°åçš„å†å²) å…ƒç»„
        
    Raises:
        ValueError: å½“ llm_interface ä¸º None æ—¶
    """
    ui = ChatbotUI()
    
    # æ‰“å°ç”¨æˆ·æ¶ˆæ¯
    ui.print_user_message(message)
    
    # ç¡®ä¿ llm_interface ä¸ä¸º None
    if llm_interface is None:
        raise ValueError("llm_interface ä¸èƒ½ä¸º None")
    
    # åˆ›å»ºèŠå¤©å‡½æ•°ï¼ˆå¯ç”¨äº‹ä»¶æµï¼‰
    @llm_chat(
        llm_interface=llm_interface,
        toolkit=[calculate, get_weather, search_knowledge],
        stream=True,
        enable_event=True,  # ğŸ”‘ å¯ç”¨äº‹ä»¶æµ
    )
    async def chat(
        user_message: str,
        chat_history: Optional[List[Dict[str, str]]] = None,
    ):
        """æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥è¿›è¡Œè®¡ç®—ã€æŸ¥è¯¢å¤©æ°”å’Œæœç´¢çŸ¥è¯†åº“ã€‚"""
        pass
    
    # å¼€å§‹å¤„ç†äº‹ä»¶æµ
    final_response = ""
    final_history: List[Dict[str, str]] = history if history is not None else []
    
    async for output in chat(user_message=message, chat_history=history):
        # ç±»å‹æ£€æŸ¥ï¼šåˆ¤æ–­æ˜¯äº‹ä»¶è¿˜æ˜¯å“åº”
        if isinstance(output, EventYield):
            # å¤„ç†äº‹ä»¶
            event = output.event
            
            if isinstance(event, ReactStartEvent):
                ui.handle_react_start(event)
            
            elif isinstance(event, LLMCallStartEvent):
                ui.handle_llm_call_start(event)
            
            elif isinstance(event, LLMChunkArriveEvent):
                ui.handle_llm_chunk(event)
                # æ›´æ–°æœ€ç»ˆå“åº”ï¼ˆä½¿ç”¨ç´¯ç§¯å†…å®¹ï¼‰
                final_response = ui.current_response
            
            elif isinstance(event, LLMCallEndEvent):
                ui.handle_llm_call_end(event)
            
            elif isinstance(event, ToolCallsBatchStartEvent):
                ui.handle_tool_calls_batch_start(event)
            
            elif isinstance(event, ToolCallStartEvent):
                ui.handle_tool_call_start(event)
            
            elif isinstance(event, ToolCallEndEvent):
                ui.handle_tool_call_end(event)
            
            elif isinstance(event, ToolCallsBatchEndEvent):
                ui.handle_tool_calls_batch_end(event)
            
            elif isinstance(event, ReactEndEvent):
                ui.handle_react_end(event)
        
        elif isinstance(output, ResponseYield):
            # å¤„ç†å“åº”æ•°æ®
            response = output.response
            # ç±»å‹è½¬æ¢ï¼šMessageList è½¬ä¸º List[Dict[str, str]]
            final_history = output.messages  # type: ignore[assignment]
            
            # æµå¼æ¨¡å¼ä¸‹ï¼Œresponse æ˜¯ LLMStreamChunkï¼Œå†…å®¹å·²ç»åœ¨ LLMChunkArriveEvent ä¸­å¤„ç†
            # éæµå¼æ¨¡å¼ä¸‹ï¼Œresponse å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ– LLMResponse å¯¹è±¡
            if isinstance(response, str):
                # éæµå¼æ¨¡å¼çš„å­—ç¬¦ä¸²å“åº”
                console.print()
                console.print("[bold green]ğŸ¤– åŠ©æ‰‹å›å¤[/bold green]")
                console.print("â”€" * 80)  # ä¸Šåˆ†éš”çº¿
                ui.render_response_chunk(response)
                console.print()  # æœ€åä¸€ä¸ª token åé¢çš„æ¢è¡Œç¬¦
                console.print("â”€" * 80)  # ä¸‹åˆ†éš”çº¿
                final_response = response
    
    # å“åº”ç»“æŸåæ¢è¡Œï¼ˆå¦‚æœæœ‰å†…å®¹è¾“å‡ºï¼‰
    if final_response:
        console.print()
    
    return final_response, final_history


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

async def main():
    """ä¸»ç¨‹åºå…¥å£"""
    # åŠ è½½ LLM é…ç½®
    try:
        models = OpenAICompatible.load_from_json_file("provider.json")
        llm = models["openrouter"]["google/gemini-3-flash-preview"]
    except Exception as e:
        console.print(f"[bold red]é”™è¯¯: æ— æ³•åŠ è½½ LLM é…ç½®: {e}[/bold red]")
        console.print("[yellow]æç¤º: è¯·ç¡®ä¿ provider.json æ–‡ä»¶å­˜åœ¨ä¸”é…ç½®æ­£ç¡®[/yellow]")
        return
    
    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    console.print("[bold cyan]SimpleLLMFunc äº‹ä»¶æµ Chatbot ç¤ºä¾‹[/bold cyan]")
    console.print()
    console.print("è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨äº‹ä»¶æµåŠŸèƒ½æ„å»ºä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„èŠå¤©æœºå™¨äººã€‚")
    console.print("ç‰¹æ€§ï¼š")
    console.print("  â€¢ å®æ—¶æµå¼å“åº”æ¸²æŸ“")
    console.print("  â€¢ å·¥å…·è°ƒç”¨å¯è§†åŒ–")
    console.print("  â€¢ å®Œæ•´çš„æ‰§è¡Œç»Ÿè®¡")
    console.print()
    console.print("å¯ç”¨å‘½ä»¤ï¼š")
    console.print("  - è¾“å…¥æ¶ˆæ¯å¼€å§‹å¯¹è¯")
    console.print("  - è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    console.print("  - è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    console.print()
    
    # å¯¹è¯å†å²
    history: List[Dict[str, str]] = []
    
    # ä¸»å¾ªç¯
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            console.print()
            user_input = console.input("[bold cyan]ä½ :[/bold cyan] ").strip()
            
            if not user_input:
                continue
            
            # å¤„ç†å‘½ä»¤
            if user_input.lower() in ["quit", "exit"]:
                console.print("[yellow]å†è§ï¼[/yellow]")
                break
            
            if user_input.lower() == "clear":
                history = []
                console.print("[yellow]å¯¹è¯å†å²å·²æ¸…ç©º[/yellow]")
                continue
            
            # å¤„ç†ç”¨æˆ·æ¶ˆæ¯
            response, history = await chatbot_with_event_ui(
                message=user_input,
                history=history,
                llm_interface=llm,
            )
        
        except KeyboardInterrupt:
            console.print("\n[yellow]ç¨‹åºå·²ä¸­æ–­[/yellow]")
            break
        
        except Exception as e:
            console.print(f"[bold red]é”™è¯¯: {e}[/bold red]")
            import traceback
            console.print(f"[dim]{traceback.format_exc()}[/dim]")


if __name__ == "__main__":
    asyncio.run(main())

